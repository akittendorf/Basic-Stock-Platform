{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"28eb0015-9c33-4e19-b523-fb3c2b01d4a8","showTitle":false,"title":""}},"source":["# LSTM Machine Learning Model"]},{"cell_type":"markdown","metadata":{},"source":["## Install Required Modules/Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0ec34680-8da8-4e60-9186-46d28f0e580b","showTitle":false,"title":""}},"outputs":[],"source":["# To install packages to our local area, use: \n","%pip install tensorflow==2.5\n","%pip install numpy==1.19.5\n","%pip install keras\n","%pip install pandas\n","%pip install matplotlib\n","%pip install sklearn\n","%pip install datetime"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"99f13bf9-ba27-4337-b9c2-ac24a3958c19","showTitle":false,"title":""}},"source":["## Imports and LSTM Data"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"44c7e612-c05b-4985-8b94-fad1e66209e1","showTitle":false,"title":""}},"outputs":[],"source":["import pandas as pd\n","from pandas import DataFrame, Series, concat, read_json\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Dropout\n","from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","import datetime\n","import math\n","import numpy as np\n","from numpy import concatenate"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a3760f8c-2d25-4b70-9160-d3a2e1a8a7b3","showTitle":false,"title":""}},"outputs":[],"source":["# Read in our cleaned and pre-formatted data into a pandas DataFrame from json format\n","with open(\"monthly_data.json\") as file:\n","    data = read_json(file)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7c52619b-d49b-46c4-9c18-c401263f3416","showTitle":false,"title":""}},"source":["## Data Handling"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e6f64e59-ff5a-45f2-b5d8-64d1e79483e6","showTitle":false,"title":""}},"outputs":[],"source":["# With our data read in, we want to focus on VTI in this model, dropping all other indexes\n","VTIDF = data.drop(['VGT', 'VIS', 'VHT', 'VFH', 'VCR'], axis=1)\n","VTIDF.info()\n","\n","# Manually order the columns into desired output, makes referencing later on more simplistic\n","col_names = ['date',\n","             'VTI',\n","             '10-Year Treasury Constant Maturity Rate',\n","             'Inflation Expectations',\n","             'Consumer Sentiment & Consumer Confidence',\n","             'Advance Retail Sales: Retail Trade',\n","             'Unemployment Rate']\n","VTIDF = VTIDF.reindex(columns = col_names)\n","cols = list(VTIDF)[1:]\n","\n","# set the dataframes index to dates for time series cases\n","VTIDF = VTIDF.set_index('date')[cols]\n","VTIDF = VTIDF.astype(float)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["VTIDF"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train test split pre-scaling, size_mult is set to 0.95, this is used when getting 95% of the data points from our base data frame\n","size_mult = 0.95\n","train_size = int(len(VTIDF) * size_mult)\n","training = VTIDF[:train_size].copy() # first 95% of data points\n","testing = VTIDF[train_size:].copy() # remaining 5% of data points\n","\n","# scaler and train test split post-scaling\n","scaler = MinMaxScaler() # creates a scaler with default end values between 0 and 1, fit individually per column\n","\n","scaled_training = scaler.fit_transform(training) # fit scaler to training data and transform the training set to normalized values\n","scaled_X_train = scaled_training # all columns including VTI\n","scaled_y_train = scaled_training[:,0]  # VTI column\n","\n","scaled_testing = scaler.transform(testing) # transform testing set to normalized values\n","scaled_X_test = scaled_testing # all columns including VTI\n","scaled_y_test = scaled_testing[:,0] # VTI column"]},{"cell_type":"markdown","metadata":{},"source":["## Time Series for ML Model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b2032e04-7b37-4ded-9020-3ea74c9d9343","showTitle":false,"title":""}},"outputs":[],"source":["# define general parameters for time series generators and ML model\n","window_length = 1 # previous data points we want to reference from when training/predicting\n","batch_size = 1\n","num_features = 6 # static number of features we are testing on, this includes our desired outcome feature 'VTI'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create ordered arrays for the train and test sets to train the ML model or test the model after training\n","# Example: x = window, o = desired outcome, - = unused data, length = x.count(), sampling_rate = o.count()\n","# gen[0]: |x|o|-|-|-|-|\n","# gen[1]: |-|x|o|-|-|-|\n","train_generator = TimeseriesGenerator(scaled_X_train, scaled_y_train, length=window_length, sampling_rate=1, batch_size=batch_size)\n","test_generator = TimeseriesGenerator(scaled_X_test, scaled_y_test, length=window_length, sampling_rate=1, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["## LSTM ML Model"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"94a40827-bfd9-45f6-b560-e1b5513cf2bf","showTitle":false,"title":""}},"outputs":[],"source":["# Model and Nodes\n","# Uses Sequenctial model type from Keras, utilizing the LSTM model.\n","# With multivariable equations, it is likely that more than one node is helpful in the training and accuracy of the model but too many can heavily impact the speed and accuracy of the model\n","model = Sequential()\n","\n","model.add(LSTM(128, activation='relu', return_sequences=True, input_shape = (window_length, num_features))) # First Node Layer, 128 nodes\n","model.add(Dropout(0.2)) # Dropout Layer to remove loosely fitted data, helps improve accuracy\n","model.add(LSTM(32, activation='relu', return_sequences=False)) # Second Node Layer, 32 nodes\n","model.add(Dropout(0.2)) # Dropout Layer\n","model.add(Dense(1)) # Dense Layer, returns output value in size (1,1)\n","\n","# designed an early stopping method in case the model begins to overfit or not improve after 10 interances, note this does not take the most efficient epoch but the last one trained\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n","\n","# compile the model using the Adam optimizer with mean-squared-error as our loss determinant\n","model.compile(optimizer='adam', loss='mse')\n","\n","# gives a summary of the amount of parameters and other helpful information of the created model\n","model.summary()\n","\n","# call fit_generator onto the model, training the model on the given TimeseriesGenerators created earlier\n","history = model.fit_generator(train_generator, validation_data=test_generator, epochs=100, shuffle = False, callbacks=[early_stopping])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# use this line to save the model to local or specific directory\n","# model.save('LSTM_ManualSplit_model.h5')\n","\n","# use this line to load a saved model from local or specific directory\n","model = keras.models.load_model('LSTM_Model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c4da3fef-4d45-40c5-9680-59b3944dd7f4","showTitle":false,"title":""}},"outputs":[],"source":["# we may care to see how training our model varies over the training sets that it iterates through. We also want to know if it becomes more accurate while not becoming overfit\n","# the main thing we are looking for in this graph is that the training loss decreases over iterations as well as the validation loss.\n","# note: if the validation loss trend is increasing over multiple tests, this means our data is being overfit and the structure of the ML model should be tweaked\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.legend()"]},{"cell_type":"markdown","metadata":{},"source":["## Model Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"31c4081c-a491-4bd8-a64c-b2390b33756c","showTitle":false,"title":""}},"outputs":[],"source":["# lets determine values outside of our training set utilizing the model we just created. Earlier, we created a test_generator which created the \n","# window and desired value arrays which are used in the training and predictions\n","test_predictions = model.predict_generator(test_generator)\n","df_test_pred = pd.concat([pd.DataFrame(test_predictions), pd.DataFrame(scaled_X_test[:,1:][window_length:])], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4391171c-c7c5-4019-b8a5-a97c9017b278","showTitle":false,"title":""}},"outputs":[],"source":["# here we determine what our model looks like when tested on our train data. This can be useful to see if overfitting occured when visualizing later on\n","train_predictions = model.predict_generator(train_generator)\n","df_train_pred = pd.concat([pd.DataFrame(train_predictions), pd.DataFrame(scaled_X_train[:,1:][window_length:])], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f7d1b33e-2d50-4a15-ae49-79fbf17def83","showTitle":false,"title":""}},"outputs":[],"source":["# we need to revert our scaled values back to the original/real-world values that we are able to visualize and compare\n","rev_trans_test = scaler.inverse_transform(df_test_pred)\n","rev_trans_train = scaler.inverse_transform(df_train_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9ab849c7-2e34-4cce-8e59-334ea292edb0","showTitle":false,"title":""}},"outputs":[],"source":["# copy x amount of rows from original dataframe to apply predicted values to, x being the length of our test predictions\n","df_final_test = VTIDF[-test_predictions.shape[0]:].copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7753fe05-9c9d-4058-8d4a-aec837a0c166","showTitle":false,"title":""}},"outputs":[],"source":["# copy x amount of rows from original dataframe to apply predicted values to, x being the length of our train predictions\n","df_final_train = VTIDF[:train_predictions.shape[0]].copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a59e1bdd-efbc-4f05-9359-2f59a1cdbbe0","showTitle":false,"title":""}},"outputs":[],"source":["# add the predictions of the test set onto the test dataframe\n","df_final_test['VTI_Pred'] = rev_trans_test[:,0]\n","df_final_test.drop(['10-Year Treasury Constant Maturity Rate',\n","             'Inflation Expectations',\n","             'Consumer Sentiment & Consumer Confidence',\n","             'Advance Retail Sales: Retail Trade',\n","             'Unemployment Rate'], axis=1,inplace=True)\n","df_final_test"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7014a767-2e5a-4a67-bcc2-f8b0fefdca22","showTitle":false,"title":""}},"outputs":[],"source":["# add predictions of the training set onto the training dataframe\n","df_final_train['VTI_Pred'] = rev_trans_train[:,0]\n","df_final_train.drop(['10-Year Treasury Constant Maturity Rate',\n","             'Inflation Expectations',\n","             'Consumer Sentiment & Consumer Confidence',\n","             'Advance Retail Sales: Retail Trade',\n","             'Unemployment Rate'], axis=1,inplace=True)\n","df_final_train"]},{"cell_type":"markdown","metadata":{},"source":["## Visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"eedd820c-5949-4a11-96cc-44c2a9d21741","showTitle":false,"title":""}},"outputs":[],"source":["# plot test set predictions compared to original values\n","df_final_test.plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4a268d1c-00bb-4bce-b78b-fff021dfc645","showTitle":false,"title":""}},"outputs":[],"source":["# plot train set predictions compared to original values\n","df_final_train.plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"98fcb13a-5a58-40a3-9314-25aa408f6199","showTitle":false,"title":""}},"outputs":[],"source":["# plot the entire VTI data alongside both prediction sets. Note, model requires one previous value to determine next hence the reason for the gap between the sets\n","plt.figure(figsize=(16,9))\n","\n","plt.title('Model')\n","plt.xlabel('Date')\n","plt.ylabel('Price')\n","\n","plt.plot(VTIDF['VTI'])\n","plt.plot(df_final_train['VTI_Pred'])\n","plt.plot(df_final_test['VTI_Pred'])\n","\n","plt.legend(['Actual Value', 'Predicted Training Value', 'Predicted Test Value'], loc='lower right')\n","\n","plt.show()"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"LSTM_Model","notebookOrigID":1536955998085682,"widgets":{}},"interpreter":{"hash":"07e5725cac378644b80d762cabd732e4b4fdafe97a7f4391306aeabf738c76ed"},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
